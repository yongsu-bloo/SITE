p_lambda=[1e-4]
n_in=[1,2,3]
n_out=[1,2,3]
nonlin='elu'
lrate=[1e-3]
batch_size=[50,100]
dim_in=[50,100,200]
dim_out=[50,100,200]
batch_norm=[0]
normalization=['divide']
experiments=10
reweight_sample=[1]
split_output=[1]
rep_weight_decay=[0]
dropout_in=1.0
dropout_out=1.0
lrate_decay=0.97
decay=0.3
optimizer='Adam'
use_p_correction=0
iterations=2000
weight_init=[0.1]
pred_output_delay=200
loss='log'
sparse=0
varsel=0
repetitions=1
val_part=0.3
p_pddm=[0.001,0.00316,0.01,0.0316,0.1,0.316,1,3.16,10,31.6,100,316,1000]
p_mid_point_mini=[0.001,0.00316,0.01,0.0316,0.1,0.316,1,3.16,10,31.6,100,316,1000]
dim_pddm=200
dim_c=200
dim_s=100
datadir='data/JOBS/'
outdir='results/noisy_1000_jobs'
dataform='noisy_1000_jobs.train.npz'
data_test='noisy_1000_jobs.test.npz'
propensity_dir='./propensity_score/noisy_1000_jobs_propensity_model.sav'
